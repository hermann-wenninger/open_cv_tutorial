{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Multithreading with Numba\n","Aufgrund der CPU-Einschränkungen bei der Ausführung von Notebooks auf Binder werden die Vorteile von Multithreading unberechenbar sein. Der Code in diesem Notebook wird auf Binder ausgeführt, aber für vernünftige Benchmarks sollten Sie dieses Notebook herunterladen und auf Ihrem eigenen System ausführen.*\n","\n","Numba unterstützt mehrere Ansätze für Multithreading:\n","\n","* Automatisches Multithreading von Array-Ausdrücken und -Reduktionen\n","* Explizites Multithreading von Schleifen mit `prange()`\n","* Externes Multithreading mit Tools wie concurrent.futures oder Dask.\n","\n","Die ersten beiden Optionen nutzen den *ParallelAccelerator*-Optimierungspass (von Intel beigesteuert) in Numba. ParallelAccelerator wird nur auf 64-Bit-Plattformen unterstützt und ist für Python 2.7 unter Windows nicht verfügbar. Es ist auch nur wirksam, wenn im Nopython-Modus kompiliert wird."]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":false},"outputs":[],"source":["import numpy as np\n","import numba\n","from numba import jit"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Automatic Multithreading\n","\n","NumPy-Array-Ausdrücke weisen eine beträchtliche implizite Parallelität auf, da Operationen unabhängig über die Eingabeelemente gesendet werden. ParallelAccelerator kann diese Parallelität erkennen und automatisch auf mehrere Threads verteilen. Alles, was wir tun müssen, ist den Parallelisierungsdurchlauf mit `parallel=True` zu ​​aktivieren:"]},{"cell_type":"code","execution_count":13,"metadata":{"trusted":false},"outputs":[],"source":["SQRT_2PI = np.sqrt(2 * np.pi)\n","\n","@jit(nopython=True, parallel=True)\n","def gaussians(x, means, widths):\n","    '''Return the value of gaussian kernels.\n","    \n","    x - location of evaluation\n","    means - array of kernel means\n","    widths - array of kernel widths\n","    '''\n","    n = means.shape[0]\n","    result = np.exp( -0.5 * ((x - means) / widths)**2 ) / widths\n","    return result / SQRT_2PI / n"]},{"cell_type":"code","execution_count":14,"metadata":{"trusted":false},"outputs":[{"data":{"text/plain":["array([1.60820340e-06, 1.41785675e-06, 1.23636617e-07, ...,\n","       2.91865528e-08, 4.55294303e-08, 2.88343262e-13])"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["means = np.random.uniform(-1, 1, size=1000000)\n","widths = np.random.uniform(0.1, 0.3, size=1000000)\n","\n","gaussians(0.4, means, widths)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Um den Effekt mehrerer CPUs zu sehen, können wir den Fall vergleichen, in dem ParallelAccelerator deaktiviert ist. In Anbetracht dessen, dass Decorators Funktionen sind, die andere Funktionen umwandeln, können wir \"jit\" als Funktion aufrufen:"]},{"cell_type":"code","execution_count":27,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["7.54 ms ± 595 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}],"source":["gaussians_nothread = jit(nopython=True)(gaussians.py_func)\n","\n","%timeit gaussians_nothread(0.4, means, widths)\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3.84 ms ± 293 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"]}],"source":["%timeit gaussians(0.4, means, widths)"]},{"cell_type":"markdown","metadata":{},"source":["We can also compare the performance to the uncompiled NumPy array evaluation using the `.py_func` attribute to get the original Python function:"]},{"cell_type":"code","execution_count":29,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["29.8 ms ± 2.02 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"]}],"source":["%timeit gaussians.py_func(0.4, means, widths) # compare to pure NumPy"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Das Leistungsverhältnis hängt von der Anzahl der CPUs in Ihrem System ab, aber die Multithread-Version ist definitiv schneller als die Single-Thread-Version.\n","\n","ParallelAccelerator kann auch mit Reduktionen umgehen:"]},{"cell_type":"code","execution_count":30,"metadata":{"trusted":false},"outputs":[],"source":["@jit(nopython=True, parallel=True)\n","def kde(x, means, widths):\n","    '''Return the value of gaussian kernels.\n","    \n","    x - location of evaluation\n","    means - array of kernel means\n","    widths - array of kernel widths\n","    '''\n","    n = means.shape[0]\n","    result = np.exp( -0.5 * ((x - means) / widths)**2 ) / widths\n","    return result.mean() / SQRT_2PI\n","\n","kde_nothread = jit(nopython=True)(kde.py_func)"]},{"cell_type":"code","execution_count":31,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["7.73 ms ± 717 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n","3.15 ms ± 545 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"]}],"source":["%timeit kde_nothread(0.4, means, widths)\n","%timeit kde(0.4, means, widths)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Multithreading with `prange()`\n","\n","Es gibt andere Situationen, in denen Sie Multithreading möchten, aber keinen einfachen Array-Ausdruck haben. In diesen Fällen zeigt die Verwendung von \"prange()\" in einer for-Schleife ParallelAccelerator an, dass dies eine Schleife ist, bei der jede Iteration unabhängig von der anderen ist und parallel ausgeführt werden kann.\n","\n","Beispielsweise möchten wir möglicherweise viele Monte-Carlo-Versuche hintereinander ausführen:"]},{"cell_type":"code","execution_count":35,"metadata":{"trusted":false},"outputs":[],"source":["import random\n","\n","# Serial version\n","@jit(nopython=True)\n","def monte_carlo_pi_serial(nsamples):\n","    acc = 0\n","    for i in range(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples\n","\n","# Parallel version\n","@jit(nopython=True, parallel=True)\n","def monte_carlo_pi_parallel(nsamples):\n","    acc = 0\n","    # Only change is here\n","    for i in numba.prange(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples\n","\n","def monte_carlo_pi_serial_o(nsamples):\n","    acc = 0\n","    for i in range(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Beachten Sie, dass `prange()` die Reduktionsvariable `acc` automatisch Thread-sicher für Sie handhabt. Wir verlassen uns auch auf Numba, um den Zufallszahlengenerator in jedem Thread unabhängig voneinander automatisch zu initialisieren.\n","\n","Sie können auch jeden Thread in einem `prange()` veranlassen, ein separates Element in einem Ausgabe-Array zu modifizieren, aber allgemeinere Race-Bedingungen werden nicht automatisch von ParallelAccelerator aufgelöst. Vorsichtig sein!\n","\n","Mal sehen, wie schnell diese beiden Implementierungen sind:"]},{"cell_type":"code","execution_count":32,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 4.8 s\n","Wall time: 4.84 s\n"]},{"data":{"text/plain":["3.1416874"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["%time monte_carlo_pi_serial(int(4e8))\n"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 12.8 s\n","Wall time: 1.91 s\n"]},{"data":{"text/plain":["3.14158536"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["%time monte_carlo_pi_parallel(int(4e8))"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: total: 6min 37s\n","Wall time: 6min 40s\n"]},{"data":{"text/plain":["3.14162044"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["%time monte_carlo_pi_serial_o(int(4e8))"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["400000000\n"]}],"source":["print(int(4e8))"]},{"cell_type":"markdown","metadata":{},"source":["The parallel version saturates all the CPUs once the initial compilation finishes."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### External Multithreading\n","\n","Manchmal befindet sich Ihr Threading-System vollständig außerhalb von Numba. Möglicherweise verwenden Sie „concurrent.futures“, um Funktionen in mehreren Threads auszuführen, oder ein paralleles Framework wie [Dask](http://dask.pydata.org/). Für diese Situationen möchten Sie ParallelAccelerator nicht verwenden, aber zulassen, dass die von Numba kompilierte Funktion gleichzeitig in verschiedenen Threads ausgeführt wird.\n","\n","Dazu soll die Numba-Funktion während der Ausführung die Global Interpreter Lock (GIL) freigeben. Dies kann mit der Option `nogil=True` zu ​​`@jit` gemacht werden.\n","\n","Machen wir unser Monte-Carlo-Beispiel noch einmal, aber mit Dask. Beachten Sie, dass Numba weiterhin die Initialisierung separater Zufallszahlengenerator-Seeds auf jedem Thread handhabt, wie es bei ParallelAccelerator der Fall war."]},{"cell_type":"code","execution_count":38,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["3.14163503\n"]}],"source":["import dask\n","import dask.delayed\n","\n","@jit(nopython=True, nogil=True)\n","def monte_carlo_pi(nsamples):\n","    acc = 0\n","    for i in range(nsamples):\n","        x = random.random()\n","        y = random.random()\n","        if (x**2 + y**2) < 1.0:\n","            acc += 1\n","    return 4.0 * acc / nsamples\n","\n","print(monte_carlo_pi(int(4e8)))\n","\n","delayed_monte_carlo_pi = dask.delayed(monte_carlo_pi)"]},{"cell_type":"markdown","metadata":{},"source":["Parallel execution:"]},{"cell_type":"code","execution_count":39,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["3.1416007775000003\n","CPU times: total: 34.6 s\n","Wall time: 9.09 s\n"]}],"source":["%%time\n","futures = [delayed_monte_carlo_pi(int(4e8)) for i in range(4)]\n","results = dask.compute(futures)[0]\n","print(sum(results)/4) # average resuts"]},{"cell_type":"markdown","metadata":{},"source":["Serial execution"]},{"cell_type":"code","execution_count":23,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["3.14161225\n","CPU times: total: 25.8 s\n","Wall time: 26 s\n"]}],"source":["%%time\n","futures = [delayed_monte_carlo_pi(int(4e8)) for i in range(4)]\n","results = dask.compute(futures, num_workers=1)[0]\n","print(sum(results)/4) # average resuts"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":false},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":2}
